{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import os\n",
    "os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"model_training.ipynb\"\n",
    "# wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mzhipeng-he\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import optim, nn\n",
    "from tqdm import tqdm\n",
    "\n",
    "from models import run_pytorch\n",
    "from data import get_datasets\n",
    "\n",
    "from models.pytorch.mlp import MLP\n",
    "from models.pytorch.tab_transformer import TabTransformer\n",
    "from models.pytorch.ft_transformer import FTTransformer\n",
    "from models.pytorch.logistic_regression import LogisticRegression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_config(model, input_dim, output_dim, categories_list, numerical_cols, device):\n",
    "    if model == \"LogisticRegression\":\n",
    "        model_config = {\n",
    "            \"input_dim\": input_dim,\n",
    "            \"output_dim\": output_dim,\n",
    "        }\n",
    "        train_config = {\n",
    "            \"epochs\": 20,\n",
    "            \"batch_size\": 512,\n",
    "            \"learning_rate\": 1e-3,\n",
    "            \"model\": \"LogisticRegression\",\n",
    "        }\n",
    "        return LogisticRegression(**model_config).to(device), train_config\n",
    "    elif model == \"MLP\":\n",
    "        model_config = {\n",
    "            \"input_dim\": input_dim,\n",
    "            \"output_dim\": output_dim,\n",
    "            \"num_hidden_layers\": 2,\n",
    "            \"hidden_layer_dims\": [64, 32],\n",
    "            \"dropout\": 0.2,\n",
    "            \"categories\": categories_list,\n",
    "            \"embedding_dim\": 8,\n",
    "            \"num_categorical_feature\": len(categories_list),\n",
    "            \"num_numerical_feature\": len(numerical_cols),\n",
    "        }\n",
    "        train_config = {\n",
    "            \"epochs\": 20,\n",
    "            \"batch_size\": 512,\n",
    "            \"learning_rate\": 1e-3,\n",
    "            \"model\": \"MLP\",\n",
    "        }\n",
    "        return MLP(**model_config).to(device), train_config\n",
    "    \n",
    "    elif model == \"TabTransformer\":\n",
    "        model_config = {\n",
    "            \"categories\": categories_list,\n",
    "            \"num_continuous\": len(numerical_cols),\n",
    "            \"dim\": 8, # can sweep\n",
    "            \"dim_out\": output_dim,\n",
    "            \"depth\": 6,\n",
    "            \"heads\": 8,\n",
    "            \"attn_dropout\": 0.2,\n",
    "            \"ff_dropout\": 0.2,\n",
    "            \"mlp_hidden_mults\": (4, 2), \n",
    "            \"mlp_act\": nn.ReLU(),\n",
    "            \"continuous_mean_std\": None,\n",
    "        }\n",
    "\n",
    "        train_config = {\n",
    "            \"epochs\": 20,\n",
    "            \"batch_size\": 512,\n",
    "            \"learning_rate\": 1e-3,\n",
    "            \"model\": \"TabTransformer\",\n",
    "        }\n",
    "        return TabTransformer(**model_config).to(device), train_config\n",
    "\n",
    "    elif model == \"FTTransformer\":\n",
    "        model_config = {\n",
    "            \"categories\": categories_list,\n",
    "            \"num_continuous\": len(numerical_cols),\n",
    "            \"dim\": 8, \n",
    "            \"dim_out\": output_dim,\n",
    "            \"depth\": 6,\n",
    "            \"heads\": 8, \n",
    "            \"attn_dropout\": 0.2, \n",
    "            \"ff_dropout\": 0.2, \n",
    "        }\n",
    "\n",
    "        train_config = {\n",
    "            \"epochs\": 20,\n",
    "            \"batch_size\": 512,\n",
    "            \"learning_rate\": 1e-3,\n",
    "            \"model\": \"FTTransformer\",\n",
    "        }\n",
    "        return FTTransformer(**model_config).to(device), train_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb_run = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Just for testing - Ablation\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# # device = torch.device(\"cpu\")\n",
    "\n",
    "# for dataset_name in [\"Adult\"]: # \"Adult\", \"Electricity\", \"Higgs\", \"KDDCup09_appetency\", \"Mushroom\"\n",
    "#     X_train, y_train, X_val, y_val, X_test, y_test, \\\n",
    "#         X_train_tensor, y_train_tensor, X_test_tensor, y_test_tensor, X_val_tensor, y_val_tensor, \\\n",
    "#         info = get_datasets.get_split_continues(dataset_name, device)\n",
    "#     for model_name in [\"MLP\"]:\n",
    "#         model, train_config = model_config(model_name, X_train.shape[1], 2, [], info.numerical_cols, device)\n",
    "#         train_config[\"dataset\"] = f\"{dataset_name}_continuous_only\"\n",
    "#         train_config[\"device\"] = device\n",
    "#         criterion = nn.CrossEntropyLoss()\n",
    "#         optimizer = run_pytorch.build_optimizer(model, \"adam\", train_config[\"learning_rate\"])\n",
    "\n",
    "#         print(f\"X_train shape: {X_train.shape}\")\n",
    "#         config = {**train_config}\n",
    "\n",
    "#         if wandb_run:\n",
    "#             with wandb.init(project=\"TabAttackBench-ModelTraining\", config=config):\n",
    "#                 run_pytorch.train(model, (X_train_tensor, y_train_tensor), (X_val_tensor, y_val_tensor), criterion, optimizer, train_config, wandb_run=wandb.run)\n",
    "#                 # torch.cuda.empty_cache() # clear GPU memory\n",
    "#                 # and test its final performance\n",
    "#                 run_pytorch.test(model, (X_test_tensor, y_test_tensor), train_config, stage=\"train\", wandb_run=wandb.run)\n",
    "#                 torch.cuda.empty_cache()\n",
    "#         else:\n",
    "#             run_pytorch.train(model, (X_train_tensor, y_train_tensor), (X_val_tensor, y_val_tensor), criterion, optimizer, train_config)\n",
    "#             # torch.cuda.empty_cache() # clear GPU memory\n",
    "#             run_pytorch.test(model, (X_test_tensor, y_test_tensor), train_config, stage=\"train\")\n",
    "#             torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for model_name in [\"MLP\"]:\n",
    "#     model, train_config = model_config(model_name, X_train.shape[1], 2, info.num_categories_list, info.numerical_cols, device)\n",
    "#     train_config[\"dataset\"] = dataset_name\n",
    "#     train_config[\"device\"] = device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LogisticRegression on WineQuality-White...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:00<00:00, 22.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running test: LogisticRegression on WineQuality-White...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:00<00:00, 890.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 67.45%\n",
      "Test results logged to WandB not available. Consider passing a WandB run object for logging.\n",
      "============= Diagnostic Run torch.onnx.export version 2.0.1+cu117 =============\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n",
      "ONNX model saved as models/train/LogisticRegression/WineQuality-White/train_run-test.onnx. Consider passing a WandB run object for saving.\n",
      "PyTorch model saved as models/train/LogisticRegression/WineQuality-White/train_run-test.pt. Consider passing a WandB run object for saving.\n",
      "Training MLP on WineQuality-White...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:00<00:00, 22.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running test: MLP on WineQuality-White...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:00<00:00, 1505.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 74.69%\n",
      "Test results logged to WandB not available. Consider passing a WandB run object for logging.\n",
      "============= Diagnostic Run torch.onnx.export version 2.0.1+cu117 =============\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n",
      "ONNX model saved as models/train/MLP/WineQuality-White/train_run-test.onnx. Consider passing a WandB run object for saving.\n",
      "PyTorch model saved as models/train/MLP/WineQuality-White/train_run-test.pt. Consider passing a WandB run object for saving.\n",
      "Training TabTransformer on WineQuality-White...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:00<00:00, 20.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running test: TabTransformer on WineQuality-White...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:00<00:00, 2108.75it/s]\n",
      "/home/zippo/projects/TabAttackBench/models/pytorch/tab_transformer.py:257: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  assert x_cont.shape[1] == self.num_continuous, f'you must pass in {self.num_continuous} values for your continuous input'\n",
      "/home/zippo/projects/TabAttackBench/models/pytorch/tab_transformer.py:268: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if not return_attn:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 73.16%\n",
      "Test results logged to WandB not available. Consider passing a WandB run object for logging.\n",
      "============= Diagnostic Run torch.onnx.export version 2.0.1+cu117 =============\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n",
      "ONNX model saved as models/train/TabTransformer/WineQuality-White/train_run-test.onnx. Consider passing a WandB run object for saving.\n",
      "PyTorch model saved as models/train/TabTransformer/WineQuality-White/train_run-test.pt. Consider passing a WandB run object for saving.\n",
      "Training FTTransformer on WineQuality-White...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:05<00:00,  3.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running test: FTTransformer on WineQuality-White...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:00<00:00, 308.31it/s]\n",
      "/home/zippo/projects/TabAttackBench/models/pytorch/ft_transformer.py:243: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if not return_attn:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 75.20%\n",
      "Test results logged to WandB not available. Consider passing a WandB run object for logging.\n",
      "============= Diagnostic Run torch.onnx.export version 2.0.1+cu117 =============\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n",
      "ONNX model saved as models/train/FTTransformer/WineQuality-White/train_run-test.onnx. Consider passing a WandB run object for saving.\n",
      "PyTorch model saved as models/train/FTTransformer/WineQuality-White/train_run-test.pt. Consider passing a WandB run object for saving.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"cpu\")\n",
    "\n",
    "for dataset_name in [\"WineQuality-White\"]: # \"Adult\", \"Electricity\", \"Higgs\", \"KDDCup09_appetency\", \"Mushroom\", \"GermanCredit\", \"BankMarketing\", \"house_16H\", \"jm1\", \"Diabetes\"\n",
    "    X_train, y_train, X_val, y_val, X_test, y_test, \\\n",
    "        X_train_tensor, y_train_tensor, X_test_tensor, y_test_tensor, X_val_tensor, y_val_tensor, \\\n",
    "        info = get_datasets.get_split(dataset_name, device)\n",
    "    for model_name in [\"LogisticRegression\", \"MLP\", \"TabTransformer\", \"FTTransformer\"]: # \"MLP\", \"TabTransformer\", \"FTTransformer\"\n",
    "        model, train_config = model_config(model_name, X_train.shape[1], 2, info.num_categories_list, info.numerical_cols, device)\n",
    "        train_config[\"dataset\"] = dataset_name\n",
    "        train_config[\"device\"] = device\n",
    "\n",
    "        if dataset_name in [\"Diabetes\", \"BreastCancer\", \"WineQuality-Red\"]:\n",
    "            train_config[\"batch_size\"] = 64\n",
    "        if dataset_name in [\"WineQuality-White\"]:\n",
    "            train_config[\"batch_size\"] = 128\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = run_pytorch.build_optimizer(model, \"adam\", train_config[\"learning_rate\"])\n",
    "\n",
    "        config = {**train_config}\n",
    "\n",
    "        if wandb_run:\n",
    "            with wandb.init(project=\"TabAttackBench-ModelTraining\", config=config):\n",
    "                run_pytorch.train(model, (X_train_tensor, y_train_tensor), (X_val_tensor, y_val_tensor), criterion, optimizer, train_config, wandb_run=wandb.run)\n",
    "                # torch.cuda.empty_cache() # clear GPU memory\n",
    "                # and test its final performance\n",
    "                run_pytorch.test(model, (X_test_tensor, y_test_tensor), train_config, stage=\"train\", wandb_run=wandb.run)\n",
    "                torch.cuda.empty_cache()\n",
    "        else:\n",
    "            run_pytorch.train(model, (X_train_tensor, y_train_tensor), (X_val_tensor, y_val_tensor), criterion, optimizer, train_config)\n",
    "            # torch.cuda.empty_cache() # clear GPU memory\n",
    "            run_pytorch.test(model, (X_test_tensor, y_test_tensor), train_config, stage=\"train\")\n",
    "            torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, data, config, stage=\"test\", wandb_run=None):\n",
    "    model_name = config[\"model\"]\n",
    "    data_name = config[\"dataset\"]\n",
    "\n",
    "    X_test_tensor, y_test_tensor = data\n",
    "\n",
    "    run_name = wandb.run.name if wandb_run else \"test\"\n",
    "    sweep_id = wandb.run.sweep_id if wandb_run and stage == \"sweep\" else stage\n",
    "\n",
    "    print(f\"Running test: {config['model']} on {config['dataset']}...\")\n",
    "    model.eval()\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(0, X_test_tensor.size(0), config[\"batch_size\"])):\n",
    "            inputs = X_test_tensor[i:i+config[\"batch_size\"]].to(config[\"device\"])\n",
    "            labels = y_test_tensor[i:i+config[\"batch_size\"]].to(config[\"device\"])\n",
    "\n",
    "            test_outputs = model(inputs)\n",
    "            # predicted = (test_outputs >= 0.5).float()\n",
    "            _, predicted = test_outputs.max(dim=1)\n",
    "\n",
    "            correct_predictions += (predicted == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "        accuracy = correct_predictions / total_samples\n",
    "        print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "        \n",
    "        if wandb_run:\n",
    "            wandb_run.log({\"test_accuracy\": accuracy})\n",
    "        else:\n",
    "            print(\"Test results logged to WandB not available. Consider passing a WandB run object for logging.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running test: LogisticRegression on GermanCredit...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 1605.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 70.50%\n",
      "Test results logged to WandB not available. Consider passing a WandB run object for logging.\n",
      "Running test: MLP on GermanCredit...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 423.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 74.00%\n",
      "Test results logged to WandB not available. Consider passing a WandB run object for logging.\n",
      "Running test: TabTransformer on GermanCredit...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 73.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 75.00%\n",
      "Test results logged to WandB not available. Consider passing a WandB run object for logging.\n",
      "Running test: FTTransformer on GermanCredit...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 79.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 70.50%\n",
      "Test results logged to WandB not available. Consider passing a WandB run object for logging.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for dataset_name in [\"GermanCredit\"]: # \"Adult\", \"Electricity\", \"Higgs\", \"BankMarketing\", \"house_16H\", \"GermanCredit\", \"jm1\", \"Diabetes\"\n",
    "    X_train, y_train, X_val, y_val, X_test, y_test, \\\n",
    "        X_train_tensor, y_train_tensor, X_test_tensor, y_test_tensor, X_val_tensor, y_val_tensor, \\\n",
    "        info = get_datasets.get_split(dataset_name, device)\n",
    "    # Considering size of Higgs dataset, we will use only 10k samples for adversarial attack.\n",
    "    if dataset_name == \"Higgs\":\n",
    "        X_test = X_test[:10000]\n",
    "        y_test = y_test[:10000]\n",
    "        X_test_tensor = X_test_tensor[:10000]\n",
    "        y_test_tensor = y_test_tensor[:10000]\n",
    "\n",
    "    for model_name in [\"LogisticRegression\", \"MLP\", \"TabTransformer\", \"FTTransformer\"]: # \"LogisticRegression\", \"MLP\", \"TabTransformer\", \"FTTransformer\"\n",
    "        model, train_config = run_pytorch.model_config(model_name, X_train.shape[1], 2, info.num_categories_list, info.numerical_cols, device)\n",
    "        train_config[\"dataset\"] = dataset_name\n",
    "        train_config[\"device\"] = device\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = run_pytorch.build_optimizer(model, \"adam\", train_config[\"learning_rate\"])\n",
    "        path = f\"models/train/{model_name}/{dataset_name}/train_run-test.pt\"\n",
    "        model.load_state_dict(torch.load(path))\n",
    "        test(model, (X_test_tensor, y_test_tensor), train_config, stage=\"train\")\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# X_train, y_train, X_val, y_val, X_test, y_test, \\\n",
    "#         X_train_tensor, y_train_tensor, X_test_tensor, y_test_tensor, X_val_tensor, y_val_tensor, \\\n",
    "#         info = get_datasets.get_dataset(\"Higgs\", device)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tabular-benchmark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
