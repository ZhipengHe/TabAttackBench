{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import os\n",
    "os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"model_training.ipynb\"\n",
    "# wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mzhipeng-he\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import optim, nn\n",
    "from tqdm import tqdm\n",
    "\n",
    "from models import run_pytorch\n",
    "from data import get_datasets\n",
    "\n",
    "from models.pytorch.mlp import MLP\n",
    "from models.pytorch.tab_transformer import TabTransformer\n",
    "from models.pytorch.ft_transformer import FTTransformer\n",
    "from models.jax.logistic_regression import LogisticRegression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_config(model, input_dim, output_dim, categories_list, numerical_cols, device):\n",
    "    if model == \"MLP\":\n",
    "        model_config = {\n",
    "            \"input_dim\": input_dim,\n",
    "            \"output_dim\": output_dim,\n",
    "            \"num_hidden_layers\": 2,\n",
    "            \"hidden_layer_dims\": [64, 32],\n",
    "            \"dropout\": 0.2,\n",
    "            \"categories\": categories_list,\n",
    "            \"embedding_dim\": 8,\n",
    "            \"num_categorical_feature\": len(categories_list),\n",
    "            \"num_numerical_feature\": len(numerical_cols),\n",
    "        }\n",
    "        train_config = {\n",
    "            \"epochs\": 20,\n",
    "            \"batch_size\": 512,\n",
    "            \"learning_rate\": 1e-3,\n",
    "            \"model\": \"MLP\",\n",
    "            \"dropout\": 0.2,\n",
    "        }\n",
    "        return MLP(**model_config).to(device), train_config\n",
    "    \n",
    "    elif model == \"TabTransformer\":\n",
    "        model_config = {\n",
    "            \"categories\": categories_list,\n",
    "            \"num_continuous\": len(numerical_cols),\n",
    "            \"dim\": 8, # can sweep\n",
    "            \"dim_out\": output_dim,\n",
    "            \"depth\": 6,\n",
    "            \"heads\": 8,\n",
    "            \"attn_dropout\": 0.2,\n",
    "            \"ff_dropout\": 0.2,\n",
    "            \"mlp_hidden_mults\": (4, 2), \n",
    "            \"mlp_act\": nn.ReLU(),\n",
    "            \"continuous_mean_std\": None,\n",
    "        }\n",
    "\n",
    "        train_config = {\n",
    "            \"epochs\": 20,\n",
    "            \"batch_size\": 128,\n",
    "            \"learning_rate\": 1e-3,\n",
    "            \"model\": \"TabTransformer\",\n",
    "        }\n",
    "        return TabTransformer(**model_config).to(device), train_config\n",
    "\n",
    "    elif model == \"FTTransformer\":\n",
    "        model_config = {\n",
    "            \"categories\": categories_list,\n",
    "            \"num_continuous\": len(numerical_cols),\n",
    "            \"dim\": 8, \n",
    "            \"dim_out\": output_dim,\n",
    "            \"depth\": 6,\n",
    "            \"heads\": 8, \n",
    "            \"attn_dropout\": 0.2, \n",
    "            \"ff_dropout\": 0.2, \n",
    "        }\n",
    "\n",
    "        train_config = {\n",
    "            \"epochs\": 20,\n",
    "            \"batch_size\": 128,\n",
    "            \"learning_rate\": 1e-3,\n",
    "            \"model\": \"FTTransformer\",\n",
    "        }\n",
    "        return FTTransformer(**model_config).to(device), train_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb_run = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [02:13<00:00,  6.70s/it]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 2.18 GiB (GPU 0; 8.00 GiB total capacity; 4.31 GiB already allocated; 0 bytes free; 6.52 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 23\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     22\u001b[0m     run_pytorch\u001b[39m.\u001b[39mtrain(model, (X_train_tensor, y_train_tensor), (X_val_tensor, y_val_tensor), criterion, optimizer, train_config)\n\u001b[0;32m---> 23\u001b[0m     run_pytorch\u001b[39m.\u001b[39;49mtest(model, (X_test_tensor, y_test_tensor), train_config, stage\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/projects/TabAttackBench/models/run_pytorch.py:114\u001b[0m, in \u001b[0;36mtest\u001b[0;34m(model, data, config, stage, wandb_run)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m    113\u001b[0m     model\u001b[39m.\u001b[39meval()\n\u001b[0;32m--> 114\u001b[0m     test_outputs \u001b[39m=\u001b[39m model(X_test_tensor)\n\u001b[1;32m    115\u001b[0m     \u001b[39m# predicted = (test_outputs >= 0.5).float()\u001b[39;00m\n\u001b[1;32m    116\u001b[0m     _, predicted \u001b[39m=\u001b[39m test_outputs\u001b[39m.\u001b[39mmax(dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/tabular-benchmark/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/projects/TabAttackBench/models/pytorch/ft_transformer.py:230\u001b[0m, in \u001b[0;36mFTTransformer.forward\u001b[0;34m(self, x, return_attn)\u001b[0m\n\u001b[1;32m    226\u001b[0m x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat((cls_tokens, x), dim \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m)\n\u001b[1;32m    228\u001b[0m \u001b[39m# attend\u001b[39;00m\n\u001b[0;32m--> 230\u001b[0m x, attns \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer(x, return_attn \u001b[39m=\u001b[39;49m \u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    232\u001b[0m \u001b[39m# get cls token\u001b[39;00m\n\u001b[1;32m    234\u001b[0m x \u001b[39m=\u001b[39m x[:, \u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/tabular-benchmark/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/projects/TabAttackBench/models/pytorch/ft_transformer.py:88\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, x, return_attn)\u001b[0m\n\u001b[1;32m     85\u001b[0m post_softmax_attns \u001b[39m=\u001b[39m []\n\u001b[1;32m     87\u001b[0m \u001b[39mfor\u001b[39;00m attn, ff \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers:\n\u001b[0;32m---> 88\u001b[0m     attn_out, post_softmax_attn \u001b[39m=\u001b[39m attn(x)\n\u001b[1;32m     89\u001b[0m     post_softmax_attns\u001b[39m.\u001b[39mappend(post_softmax_attn)\n\u001b[1;32m     91\u001b[0m     x \u001b[39m=\u001b[39m attn_out \u001b[39m+\u001b[39m x\n",
      "File \u001b[0;32m~/miniconda3/envs/tabular-benchmark/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/projects/TabAttackBench/models/pytorch/ft_transformer.py:52\u001b[0m, in \u001b[0;36mAttention.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     49\u001b[0m q, k, v \u001b[39m=\u001b[39m \u001b[39mmap\u001b[39m(\u001b[39mlambda\u001b[39;00m t: rearrange(t, \u001b[39m'\u001b[39m\u001b[39mb n (h d) -> b h n d\u001b[39m\u001b[39m'\u001b[39m, h \u001b[39m=\u001b[39m h), (q, k, v))\n\u001b[1;32m     50\u001b[0m q \u001b[39m=\u001b[39m q \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscale\n\u001b[0;32m---> 52\u001b[0m sim \u001b[39m=\u001b[39m einsum(\u001b[39m'\u001b[39;49m\u001b[39mb h i d, b h j d -> b h i j\u001b[39;49m\u001b[39m'\u001b[39;49m, q, k)\n\u001b[1;32m     54\u001b[0m attn \u001b[39m=\u001b[39m sim\u001b[39m.\u001b[39msoftmax(dim \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     55\u001b[0m dropped_attn \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(attn)\n",
      "File \u001b[0;32m~/miniconda3/envs/tabular-benchmark/lib/python3.10/site-packages/torch/functional.py:378\u001b[0m, in \u001b[0;36meinsum\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    373\u001b[0m     \u001b[39mreturn\u001b[39;00m einsum(equation, \u001b[39m*\u001b[39m_operands)\n\u001b[1;32m    375\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(operands) \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m2\u001b[39m \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m opt_einsum\u001b[39m.\u001b[39menabled:\n\u001b[1;32m    376\u001b[0m     \u001b[39m# the path for contracting 0 or 1 time(s) is already optimized\u001b[39;00m\n\u001b[1;32m    377\u001b[0m     \u001b[39m# or the user has disabled using opt_einsum\u001b[39;00m\n\u001b[0;32m--> 378\u001b[0m     \u001b[39mreturn\u001b[39;00m _VF\u001b[39m.\u001b[39;49meinsum(equation, operands)  \u001b[39m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    380\u001b[0m path \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    381\u001b[0m \u001b[39mif\u001b[39;00m opt_einsum\u001b[39m.\u001b[39mis_available():\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 2.18 GiB (GPU 0; 8.00 GiB total capacity; 4.31 GiB already allocated; 0 bytes free; 6.52 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"cpu\")\n",
    "\n",
    "for dataset_name in [\"Adult\"]: # \"Adult\", \"Electricity\", \"Higgs\", \"KDDCup09_appetency\", \"Mushroom\"\n",
    "    X_train, y_train, X_val, y_val, X_test, y_test, \\\n",
    "        X_train_tensor, y_train_tensor, X_test_tensor, y_test_tensor, X_val_tensor, y_val_tensor, \\\n",
    "        info = get_datasets.get_split(dataset_name, device)\n",
    "    for model_name in [\"FTTransformer\"]:\n",
    "        model, train_config = model_config(model_name, X_train.shape[1], 2, info.num_categories_list, info.numerical_cols, device)\n",
    "        train_config[\"dataset\"] = dataset_name\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = run_pytorch.build_optimizer(model, \"adam\", train_config[\"learning_rate\"])\n",
    "\n",
    "        config = {**train_config}\n",
    "\n",
    "        if wandb_run:\n",
    "            with wandb.init(project=\"TabAttackBench-ModelTraining\", config=config):\n",
    "                run_pytorch.train(model, (X_train_tensor, y_train_tensor), (X_val_tensor, y_val_tensor), criterion, optimizer, train_config, wandb_run=wandb.run)\n",
    "                # and test its final performance\n",
    "                run_pytorch.test(model, (X_test_tensor, y_test_tensor), train_config, stage=\"train\", wandb_run=wandb.run)\n",
    "        else:\n",
    "            run_pytorch.train(model, (X_train_tensor, y_train_tensor), (X_val_tensor, y_val_tensor), criterion, optimizer, train_config)\n",
    "            run_pytorch.test(model, (X_test_tensor, y_test_tensor), train_config, stage=\"train\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "X_train, y_train, X_val, y_val, X_test, y_test, \\\n",
    "        X_train_tensor, y_train_tensor, X_test_tensor, y_test_tensor, X_val_tensor, y_val_tensor, \\\n",
    "        info = get_datasets.get_split(\"Adult\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info.ohe_feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# X_train, y_train, X_val, y_val, X_test, y_test, \\\n",
    "#         X_train_tensor, y_train_tensor, X_test_tensor, y_test_tensor, X_val_tensor, y_val_tensor, \\\n",
    "#         info = get_datasets.get_dataset(\"Higgs\", device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon_values = np.arange(0.01, 0.21, 0.03)\n",
    "epsilon_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(epsilon_values, attack_success_rates, marker='o', linestyle='-')\n",
    "plt.title('Success Rate vs. Epsilon')\n",
    "plt.xlabel('Epsilon')\n",
    "plt.ylabel('Success Rate')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(epsilon_values, average_l2_distances, marker='o', linestyle='-')\n",
    "plt.title('L2 Distance vs. Epsilon')\n",
    "plt.xlabel('Epsilon')\n",
    "plt.ylabel('L2 Distance')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(average_l2_distances, attack_success_rates, marker='o', linestyle='-')\n",
    "plt.title('L2 Distance vs. Success Rate')\n",
    "plt.xlabel('L2 Distance')\n",
    "plt.ylabel('Success Rate')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tabular-benchmark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
