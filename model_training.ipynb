{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import os\n",
    "os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"model_training.ipynb\"\n",
    "# wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mzhipeng-he\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import optim, nn\n",
    "from tqdm import tqdm\n",
    "\n",
    "from models import run_pytorch\n",
    "from data import get_datasets\n",
    "\n",
    "from models.pytorch.mlp import MLP\n",
    "from models.pytorch.tab_transformer import TabTransformer\n",
    "from models.pytorch.ft_transformer import FTTransformer\n",
    "from models.pytorch.logistic_regression import LogisticRegression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_config(model, input_dim, output_dim, categories_list, numerical_cols, device):\n",
    "    if model == \"LogisticRegression\":\n",
    "        model_config = {\n",
    "            \"input_dim\": input_dim,\n",
    "            \"output_dim\": output_dim,\n",
    "        }\n",
    "        train_config = {\n",
    "            \"epochs\": 20,\n",
    "            \"batch_size\": 512,\n",
    "            \"learning_rate\": 1e-3,\n",
    "            \"model\": \"LogisticRegression\",\n",
    "        }\n",
    "        return LogisticRegression(**model_config).to(device), train_config\n",
    "    elif model == \"MLP\":\n",
    "        model_config = {\n",
    "            \"input_dim\": input_dim,\n",
    "            \"output_dim\": output_dim,\n",
    "            \"num_hidden_layers\": 2,\n",
    "            \"hidden_layer_dims\": [64, 32],\n",
    "            \"dropout\": 0.2,\n",
    "            \"categories\": categories_list,\n",
    "            \"embedding_dim\": 8,\n",
    "            \"num_categorical_feature\": len(categories_list),\n",
    "            \"num_numerical_feature\": len(numerical_cols),\n",
    "        }\n",
    "        train_config = {\n",
    "            \"epochs\": 20,\n",
    "            \"batch_size\": 512,\n",
    "            \"learning_rate\": 1e-3,\n",
    "            \"model\": \"MLP\",\n",
    "            \"dropout\": 0.2,\n",
    "        }\n",
    "        return MLP(**model_config).to(device), train_config\n",
    "    \n",
    "    elif model == \"TabTransformer\":\n",
    "        model_config = {\n",
    "            \"categories\": categories_list,\n",
    "            \"num_continuous\": len(numerical_cols),\n",
    "            \"dim\": 8, # can sweep\n",
    "            \"dim_out\": output_dim,\n",
    "            \"depth\": 6,\n",
    "            \"heads\": 8,\n",
    "            \"attn_dropout\": 0.2,\n",
    "            \"ff_dropout\": 0.2,\n",
    "            \"mlp_hidden_mults\": (4, 2), \n",
    "            \"mlp_act\": nn.ReLU(),\n",
    "            \"continuous_mean_std\": None,\n",
    "        }\n",
    "\n",
    "        train_config = {\n",
    "            \"epochs\": 20,\n",
    "            \"batch_size\": 512,\n",
    "            \"learning_rate\": 1e-3,\n",
    "            \"model\": \"TabTransformer\",\n",
    "        }\n",
    "        return TabTransformer(**model_config).to(device), train_config\n",
    "\n",
    "    elif model == \"FTTransformer\":\n",
    "        model_config = {\n",
    "            \"categories\": categories_list,\n",
    "            \"num_continuous\": len(numerical_cols),\n",
    "            \"dim\": 8, \n",
    "            \"dim_out\": output_dim,\n",
    "            \"depth\": 6,\n",
    "            \"heads\": 8, \n",
    "            \"attn_dropout\": 0.2, \n",
    "            \"ff_dropout\": 0.2, \n",
    "        }\n",
    "\n",
    "        train_config = {\n",
    "            \"epochs\": 20,\n",
    "            \"batch_size\": 512,\n",
    "            \"learning_rate\": 1e-3,\n",
    "            \"model\": \"FTTransformer\",\n",
    "        }\n",
    "        return FTTransformer(**model_config).to(device), train_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb_run = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Just for testing - Ablation\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# # device = torch.device(\"cpu\")\n",
    "\n",
    "# for dataset_name in [\"Adult\"]: # \"Adult\", \"Electricity\", \"Higgs\", \"KDDCup09_appetency\", \"Mushroom\"\n",
    "#     X_train, y_train, X_val, y_val, X_test, y_test, \\\n",
    "#         X_train_tensor, y_train_tensor, X_test_tensor, y_test_tensor, X_val_tensor, y_val_tensor, \\\n",
    "#         info = get_datasets.get_split_continues(dataset_name, device)\n",
    "#     for model_name in [\"MLP\"]:\n",
    "#         model, train_config = model_config(model_name, X_train.shape[1], 2, [], info.numerical_cols, device)\n",
    "#         train_config[\"dataset\"] = f\"{dataset_name}_continuous_only\"\n",
    "#         train_config[\"device\"] = device\n",
    "#         criterion = nn.CrossEntropyLoss()\n",
    "#         optimizer = run_pytorch.build_optimizer(model, \"adam\", train_config[\"learning_rate\"])\n",
    "\n",
    "#         print(f\"X_train shape: {X_train.shape}\")\n",
    "#         config = {**train_config}\n",
    "\n",
    "#         if wandb_run:\n",
    "#             with wandb.init(project=\"TabAttackBench-ModelTraining\", config=config):\n",
    "#                 run_pytorch.train(model, (X_train_tensor, y_train_tensor), (X_val_tensor, y_val_tensor), criterion, optimizer, train_config, wandb_run=wandb.run)\n",
    "#                 # torch.cuda.empty_cache() # clear GPU memory\n",
    "#                 # and test its final performance\n",
    "#                 run_pytorch.test(model, (X_test_tensor, y_test_tensor), train_config, stage=\"train\", wandb_run=wandb.run)\n",
    "#                 torch.cuda.empty_cache()\n",
    "#         else:\n",
    "#             run_pytorch.train(model, (X_train_tensor, y_train_tensor), (X_val_tensor, y_val_tensor), criterion, optimizer, train_config)\n",
    "#             # torch.cuda.empty_cache() # clear GPU memory\n",
    "#             run_pytorch.test(model, (X_test_tensor, y_test_tensor), train_config, stage=\"train\")\n",
    "#             torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for model_name in [\"MLP\"]:\n",
    "#     model, train_config = model_config(model_name, X_train.shape[1], 2, info.num_categories_list, info.numerical_cols, device)\n",
    "#     train_config[\"dataset\"] = dataset_name\n",
    "#     train_config[\"device\"] = device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LogisticRegression on Adult...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:02<00:00,  8.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running test: LogisticRegression on Adult...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:00<00:00, 2026.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 83.40%\n",
      "Test results logged to WandB not available. Consider passing a WandB run object for logging.\n",
      "============= Diagnostic Run torch.onnx.export version 2.0.1+cu117 =============\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n",
      "ONNX model saved as models/train/LogisticRegression/Adult/train_run-test.onnx. Consider passing a WandB run object for saving.\n",
      "PyTorch model saved as models/train/LogisticRegression/Adult/train_run-test.pt. Consider passing a WandB run object for saving.\n",
      "Training LogisticRegression on Electricity...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:01<00:00, 11.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running test: LogisticRegression on Electricity...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:00<00:00, 2428.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 66.07%\n",
      "Test results logged to WandB not available. Consider passing a WandB run object for logging.\n",
      "============= Diagnostic Run torch.onnx.export version 2.0.1+cu117 =============\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n",
      "ONNX model saved as models/train/LogisticRegression/Electricity/train_run-test.onnx. Consider passing a WandB run object for saving.\n",
      "PyTorch model saved as models/train/LogisticRegression/Electricity/train_run-test.pt. Consider passing a WandB run object for saving.\n",
      "Training LogisticRegression on Higgs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:34<00:00,  1.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running test: LogisticRegression on Higgs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:00<00:00, 2689.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 63.15%\n",
      "Test results logged to WandB not available. Consider passing a WandB run object for logging.\n",
      "============= Diagnostic Run torch.onnx.export version 2.0.1+cu117 =============\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n",
      "ONNX model saved as models/train/LogisticRegression/Higgs/train_run-test.onnx. Consider passing a WandB run object for saving.\n",
      "PyTorch model saved as models/train/LogisticRegression/Higgs/train_run-test.pt. Consider passing a WandB run object for saving.\n",
      "Training LogisticRegression on Mushroom...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:00<00:00, 58.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running test: LogisticRegression on Mushroom...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 1607.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 94.89%\n",
      "Test results logged to WandB not available. Consider passing a WandB run object for logging.\n",
      "============= Diagnostic Run torch.onnx.export version 2.0.1+cu117 =============\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n",
      "ONNX model saved as models/train/LogisticRegression/Mushroom/train_run-test.onnx. Consider passing a WandB run object for saving.\n",
      "PyTorch model saved as models/train/LogisticRegression/Mushroom/train_run-test.pt. Consider passing a WandB run object for saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"cpu\")\n",
    "\n",
    "for dataset_name in [\"Adult\", \"Electricity\", \"Higgs\", \"Mushroom\"]: # \"Adult\", \"Electricity\", \"Higgs\", \"KDDCup09_appetency\", \"Mushroom\"\n",
    "    X_train, y_train, X_val, y_val, X_test, y_test, \\\n",
    "        X_train_tensor, y_train_tensor, X_test_tensor, y_test_tensor, X_val_tensor, y_val_tensor, \\\n",
    "        info = get_datasets.get_split(dataset_name, device)\n",
    "    for model_name in [\"LogisticRegression\"]: # \"MLP\", \"TabTransformer\", \"FTTransformer\"\n",
    "        model, train_config = model_config(model_name, X_train.shape[1], 2, info.num_categories_list, info.numerical_cols, device)\n",
    "        train_config[\"dataset\"] = dataset_name\n",
    "        train_config[\"device\"] = device\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = run_pytorch.build_optimizer(model, \"adam\", train_config[\"learning_rate\"])\n",
    "\n",
    "        config = {**train_config}\n",
    "\n",
    "        if wandb_run:\n",
    "            with wandb.init(project=\"TabAttackBench-ModelTraining\", config=config):\n",
    "                run_pytorch.train(model, (X_train_tensor, y_train_tensor), (X_val_tensor, y_val_tensor), criterion, optimizer, train_config, wandb_run=wandb.run)\n",
    "                # torch.cuda.empty_cache() # clear GPU memory\n",
    "                # and test its final performance\n",
    "                run_pytorch.test(model, (X_test_tensor, y_test_tensor), train_config, stage=\"train\", wandb_run=wandb.run)\n",
    "                torch.cuda.empty_cache()\n",
    "        else:\n",
    "            run_pytorch.train(model, (X_train_tensor, y_train_tensor), (X_val_tensor, y_val_tensor), criterion, optimizer, train_config)\n",
    "            # torch.cuda.empty_cache() # clear GPU memory\n",
    "            run_pytorch.test(model, (X_test_tensor, y_test_tensor), train_config, stage=\"train\")\n",
    "            torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# X_train, y_train, X_val, y_val, X_test, y_test, \\\n",
    "#         X_train_tensor, y_train_tensor, X_test_tensor, y_test_tensor, X_val_tensor, y_val_tensor, \\\n",
    "#         info = get_datasets.get_dataset(\"Higgs\", device)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tabular-benchmark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
