{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Generate adversarial examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import requests\n",
    "from io import StringIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13420/3605132819.py:44: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[categorical_cols + numerical_cols] = scaler.fit_transform(X[categorical_cols + numerical_cols])\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mzhipeng-he\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "import os\n",
    "os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"model_hyperparameter_search.ipynb\"\n",
    "wandb.login()\n",
    "\n",
    "from torch import optim, nn\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import run_pytorch\n",
    "from data import get_datasets\n",
    "\n",
    "from models.pytorch.mlp import MLP\n",
    "from models.pytorch.tab_transformer import TabTransformer\n",
    "from models.pytorch.ft_transformer import FTTransformer\n",
    "from models.jax.logistic_regression import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import foolbox\n",
    "import numpy as np\n",
    "from scipy.stats import chi2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics(adv_numpy, sample_numpy, train_mean, train_cov_matrix, threshold, std_numpy):\n",
    "\n",
    "    def _calculate_mahalanobis_distance(x, mean, cov_matrix):\n",
    "        # Calculate the Mahalanobis distance\n",
    "        diff = x - mean\n",
    "        inv_cov_matrix = np.linalg.inv(cov_matrix)\n",
    "        md = np.sqrt(np.dot(np.dot(diff, inv_cov_matrix), diff.T))\n",
    "        return md[0, 0] # return a scalar\n",
    "    \n",
    "    eps = 1e-8\n",
    "\n",
    "    # sparsity\n",
    "    l0_distance = np.count_nonzero(adv_numpy - sample_numpy)\n",
    "\n",
    "    # proximity\n",
    "    l1_distance = np.linalg.norm(adv_numpy - sample_numpy, ord=1)\n",
    "    l2_distance = np.linalg.norm(adv_numpy - sample_numpy, ord=2)\n",
    "    linf_distance = np.linalg.norm(adv_numpy - sample_numpy, ord=np.inf)\n",
    "\n",
    "    # deviation\n",
    "    md = _calculate_mahalanobis_distance(adv_numpy, train_mean, train_cov_matrix)\n",
    "    # Check if the Mahalanobis distance exceeds the threshold\n",
    "    is_outlier = md > threshold\n",
    "\n",
    "    # sensitivity: \n",
    "    sens = (l1_distance / ((std_numpy + eps) * l0_distance)).mean()\n",
    "\n",
    "    return {\n",
    "        \"L0 Distance\": l0_distance,\n",
    "        \"L1 Distance\": l1_distance,\n",
    "        \"L2 Distance\": l2_distance,\n",
    "        \"Linf Distance\": linf_distance,\n",
    "        \"Mahalanobis Distance\": md,\n",
    "        \"Is Outlier\": is_outlier,\n",
    "        \"Sensitivity\": sens,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attack_list = {\n",
    "    \"L2CarliniWagner\": foolbox.attacks.L2CarliniWagnerAttack(),\n",
    "    \"L2DeepFool\": foolbox.attacks.L2DeepFoolAttack(),\n",
    "    \"LinfFGSM\": foolbox.attacks.LinfFastGradientAttack(),\n",
    "    \"LinfPGD\": foolbox.attacks.L2CarliniWagnerAttack(),\n",
    "    \"LinfBIM\": foolbox.attacks.LinfBasicIterativeAttack(),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def attack(model, X_test_tensor, y_test_tensor, X_train_tensor):\n",
    "    # Create a Foolbox model wrapper for the PyTorch model\n",
    "    fmodel = foolbox.models.PyTorchModel(model, bounds=(0, 1))\n",
    "\n",
    "    # Create an attack object (FGSM in this case)\n",
    "    attack = foolbox.attacks.LinfFastGradientAttack()\n",
    "\n",
    "\n",
    "    # Generate an adversarial example\n",
    "    epsilons = np.arange(0.01, 0.10, 0.03)  # You can change the epsilon values\n",
    "\n",
    "    attack_success_rates = []\n",
    "    average_distances = {\n",
    "        \"L0 Distance\": [], \"L1 Distance\": [], \"L2 Distance\": [], \"Linf Distance\": [], \"Mahalanobis Distance\": [], \"Sensitivity\": []\n",
    "    }\n",
    "    outliner_rates = []\n",
    "\n",
    "    X_train_numpy = X_train_tensor.cpu().numpy()\n",
    "\n",
    "    train_mean = np.mean(X_train_numpy, axis=0)\n",
    "    train_cov_matrix = np.cov(X_train_numpy.T)\n",
    "    std = np.std(X_train_numpy, axis=0)\n",
    "\n",
    "\n",
    "    # Calculate the critical value for the Mahalanobis distance using chi-squared distribution\n",
    "    alpha = 0.05\n",
    "    degrees_of_freedom = len(train_mean)\n",
    "    # Calculate the Chi-Square critical value at the given alpha and df\n",
    "    chi_square_critical_value  = chi2.ppf(1 - alpha, df=degrees_of_freedom)\n",
    "    threshold = np.sqrt(chi_square_critical_value)\n",
    "    print(f\"Threshold: {threshold}\")\n",
    "\n",
    "    for epsilon in epsilons:\n",
    "        # Initialize variables to keep track of success and the number of samples\n",
    "        total_samples = len(X_test_tensor)\n",
    "        successful_attacks = 0\n",
    "        total_metrics = {\n",
    "            \"L0 Distance\": 0.0, \"L1 Distance\": 0.0, \"L2 Distance\": 0.0,\n",
    "            \"Linf Distance\": 0.0, \"Mahalanobis Distance\": 0.0,\n",
    "            \"Is Outlier\": 0, \"Sensitivity\": 0.0\n",
    "        }\n",
    "\n",
    "        for sample_idx in tqdm(range(total_samples)):\n",
    "            sample = X_test_tensor[sample_idx].unsqueeze(0)\n",
    "            label = y_test_tensor[sample_idx].unsqueeze(0)\n",
    "\n",
    "            _, advs, success = attack(fmodel, sample, label, epsilons=[epsilon])\n",
    "\n",
    "            with torch.no_grad():\n",
    "                model.eval()\n",
    "                test_outputs = model(sample)\n",
    "                adv_outputs = model(advs[0])\n",
    "                _, predicted_classes = test_outputs.max(dim=1)\n",
    "                _, adv_predicted_classes = adv_outputs.max(dim=1)\n",
    "\n",
    "            adv_numpy = advs[0].cpu().numpy()\n",
    "            sample_numpy = sample.cpu().numpy()\n",
    "\n",
    "            metrics_dict = metrics(adv_numpy, sample_numpy, train_mean, train_cov_matrix, threshold, std)\n",
    "\n",
    "            if success:\n",
    "                successful_attacks += 1\n",
    "\n",
    "                for key, value in metrics_dict.items():\n",
    "                    total_metrics[key] += value\n",
    "\n",
    "            # print(f\"Epsilon = {epsilon}, Predicted class: {predicted_classes} | Adversarial example: {adv_predicted_classes} | Success: {success} | L2 distance: {np.linalg.norm(advs[0].cpu().numpy() - sample.cpu().numpy())}\")\n",
    "\n",
    "        if successful_attacks > 0:\n",
    "            success_rate = successful_attacks / total_samples\n",
    "            outliner_rate = total_metrics[\"Is Outlier\"] / successful_attacks\n",
    "            for key, value in total_metrics.items():\n",
    "                if key in average_distances:\n",
    "                    average_distances[key].append(value / successful_attacks)\n",
    "\n",
    "        else:\n",
    "            success_rate = 0.0\n",
    "            outliner_rate = 0.0\n",
    "            for key, value in total_metrics.items():\n",
    "                if key in average_distances:\n",
    "                    average_distances[key].append(0.0)\n",
    "        \n",
    "        attack_success_rates.append(success_rate)\n",
    "        outliner_rates.append(outliner_rate)\n",
    "        \n",
    "        print(f\"Epsilon = {epsilon}\")\n",
    "        print(f\"Success Rate: {success_rate * 100}%\")\n",
    "        for key, value in average_distances.items():\n",
    "            print(f\"Average {key} for Successful Attacks: {value[-1]}\")\n",
    "        print(f\"Outlier Rate for Successful Attacks: {outliner_rate * 100}%\\n\")\n",
    "        print(\"\")\n",
    "\n",
    "    return attack_success_rates, average_distances, outliner_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(dataset_name, model_name):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    X_train, y_train, X_val, y_val, X_test, y_test, \\\n",
    "        X_train_tensor, y_train_tensor, X_test_tensor, y_test_tensor, X_val_tensor, y_val_tensor, \\\n",
    "        info = get_datasets.get_dataset(dataset_name, device)\n",
    "    \n",
    "    model, train_config = model_config(model_name, X_train.shape[1], 2, info[\"categories_list\"], info[\"numerical_cols\"], device)\n",
    "\n",
    "    train_config[\"dataset\"] = dataset_name\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = run_pytorch.build_optimizer(model, \"adam\", train_config[\"learning_rate\"])\n",
    "    \n",
    "    run_pytorch.train(model, (X_train_tensor, y_train_tensor), (X_val_tensor, y_val_tensor), criterion, optimizer, train_config)\n",
    "\n",
    "    # and test its final performance\n",
    "    run_pytorch.test(model, (X_test_tensor, y_test_tensor), train_config, stage=\"train\", wandb_run=wandb.run)\n",
    "\n",
    "    attack_success_rates, average_distances, outliner_rates = attack(model, X_test_tensor, y_test_tensor, X_train_tensor)\n",
    "\n",
    "    return attack_success_rates, average_distances, outliner_rates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb.log({\"attack_success_rates\": attack_success_rates, \"average_l2_distances\": average_l2_distances, \"dataset\": dataset_name, \"model\": model_name})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tabular-benchmark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
