{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5: Results Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import os\n",
    "os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"result_evaluation.ipynb\"\n",
    "wandb.login()\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# campare the feature difference between the original and perturbated samples\n",
    "def calculate_perturbated_feature(arr1, arr2):\n",
    "    # Create a boolean matrix where True indicates elements are different\n",
    "    diff_matrix = arr1 != arr2\n",
    "    \n",
    "    # Convert boolean matrix to integer matrix (1 for True, 0 for False)\n",
    "    result_matrix = diff_matrix.astype(int)\n",
    "    \n",
    "    return result_matrix.sum(axis=0)\n",
    "\n",
    "# Load attack results from pickle file to dictionary\n",
    "def load_results(dataset_name, model_name, attack_name):\n",
    "\n",
    "    file_path = f\"results/{dataset_name}_{model_name}_{attack_name}.pickle\"\n",
    "\n",
    "    # if file does not exist, return empty dictionary\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"The file 'results/{dataset_name}_{model_name}_{attack_name}.pickle' does not exist. Skip.\")\n",
    "        return None\n",
    "\n",
    "    # Load the pickled object from the file\n",
    "    with open(file_path, 'rb') as handle:\n",
    "        loaded_results = pickle.load(handle)\n",
    "\n",
    "\n",
    "    # Ensure the loaded object is a dictionary\n",
    "    if isinstance(loaded_results, dict):\n",
    "        my_dict = loaded_results\n",
    "        # print(\"Successfully loaded the pickled dictionary.\")\n",
    "        return my_dict\n",
    "    else:\n",
    "        print(\"The loaded object is not a dictionary.\")\n",
    "        raise TypeError(f\"The loaded object is not a dictionary, which is {type(my_dict)}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load pickle files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Adult\", \"Electricity\", \"Higgs\", \"BankMarketing\", \"house_16H\", \"GermanCredit\", \"jm1\", \"BreastCancer\"\n",
    "dataset_list = [\"Adult\", \"Electricity\", \"Higgs\", \"BankMarketing\", \"house_16H\", \"GermanCredit\", \"jm1\", \"BreastCancer\"]\n",
    "model_list = [\"LogisticRegression\",\"MLP\", \"TabTransformer\", \"FTTransformer\"]\n",
    "attack_list = [\"L2Gaussian\", \"L2Uniform\", \"LinfUniform\", \"LinfFGSM\", \"LinfPGD\", \"LinfBIM\", \"L2CarliniWagner\", \"L2DeepFool\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = dict()\n",
    "for dataset_name in dataset_list:\n",
    "    for model_name in model_list:\n",
    "        for attack_name in attack_list:\n",
    "            # Load the results from pickle file\n",
    "            result = load_results(dataset_name, model_name, attack_name)\n",
    "            all_results[f\"{dataset_name}_{model_name}_{attack_name}\"] = result\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"Adult\"\n",
    "model_name = \"MLP\"\n",
    "attack_name = \"LinfPGD\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attack_success_rates = all_results[f\"{dataset_name}_{model_name}_{attack_name}\"][\"result\"]['attack_success_rates']\n",
    "average_distances = all_results[f\"{dataset_name}_{model_name}_{attack_name}\"][\"result\"]['average_distances']\n",
    "l0_distances = average_distances['L0 Distance']\n",
    "l1_distances = average_distances['L1 Distance']\n",
    "l2_distances = average_distances['L2 Distance']\n",
    "linf_distances = average_distances['Linf Distance']\n",
    "mahalanobis_distances = average_distances['Mahalanobis Distance']\n",
    "sensitivity = average_distances['Sensitivity']\n",
    "outliner_rates = all_results[f\"{dataset_name}_{model_name}_{attack_name}\"][\"result\"]['outliner_rates']\n",
    "epsilons = all_results[f\"{dataset_name}_{model_name}_{attack_name}\"][\"result\"]['epsilons']\n",
    "paths = all_results[f\"{dataset_name}_{model_name}_{attack_name}\"][\"result\"]['numpy_path']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Plot the relationship between epsilon and attack success rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the relationship between epsilon and attack success rate\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(epsilons, attack_success_rates, marker='o', linestyle='-')\n",
    "plt.title(f'Model: {model_name} - Dataset: {dataset_name} - Attack: {attack_name} - Success Rate vs. Epsilon')\n",
    "plt.xlabel('Epsilon')\n",
    "plt.ylabel('Success Rate')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Plot the relationship between epsilon and distances metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linf distance\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(epsilons, linf_distances, marker='o', linestyle='-')\n",
    "plt.title(f'Model: {model_name} - Dataset: {dataset_name} - Attack: {attack_name} - Linf Distance vs. Epsilon')\n",
    "plt.xlabel('Epsilon')\n",
    "plt.ylabel('Linf Distance')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L2 distance\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(epsilons, l2_distances, marker='o', linestyle='-')\n",
    "plt.title(f'Model: {model_name} - Dataset: {dataset_name} - Attack: {attack_name} - L2 Distance vs. Epsilon')\n",
    "plt.xlabel('Epsilon')\n",
    "plt.ylabel('L2 Distance')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L1 distance\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(epsilons, l1_distances, marker='o', linestyle='-')\n",
    "plt.title(f'Model: {model_name} - Dataset: {dataset_name} - Attack: {attack_name} - L1 Distance vs. Epsilon')\n",
    "# plt.ylim(0,6)\n",
    "plt.xlabel('Epsilon')\n",
    "plt.ylabel('L1 Distance')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Plot the relationship between epsilon and sparsity metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L0 distance\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(epsilons, l0_distances, marker='o', linestyle='-')\n",
    "plt.title(f'Model: {model_name} - Dataset: {dataset_name} - Attack: {attack_name} - Sparsity vs. Epsilon')\n",
    "# plt.ylim(0,6)\n",
    "plt.xlabel('Epsilon')\n",
    "plt.ylabel('L0 Distance')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Plot relationship between L2 distance vs. attack success rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L2 distance vs. attack success rate\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(l2_distances, attack_success_rates, marker='o', linestyle='-')\n",
    "plt.title(f'Model: {model_name} - Dataset: {dataset_name} - Attack: {attack_name} - L2 Distance vs. Success Rate')\n",
    "plt.xlabel('L2 Distance')\n",
    "plt.ylabel('Success Rate')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. What feature are perturbed the most?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_list = []\n",
    "for i in range(len(epsilons)):\n",
    "    success_arr = np.load(paths[i][\"success_arr\"])\n",
    "    success_arr_adv = np.load(paths[i][\"success_arr_adv\"])\n",
    "\n",
    "    matrix = calculate_perturbated_feature(success_arr, success_arr_adv)\n",
    "    matrix_list.append(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a heatmap using matplotlib\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.imshow(matrix_list, cmap='viridis',aspect='auto')\n",
    "# Add grid lines for each cell\n",
    "plt.grid(which='both', color='grey', linewidth=0.1)\n",
    "plt.colorbar(label='Values')\n",
    "plt.title(f'Model: {model_name} - Dataset: {dataset_name} - Attack: {attack_name} Heatmap of Perturbated Features')\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Epsilon')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use subplot to plot all the figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_in_grid(all_results, dataset_name, model_name, attack_name):\n",
    "    attack_success_rates = all_results[f\"{dataset_name}_{model_name}_{attack_name}\"][\"result\"]['attack_success_rates']\n",
    "    average_distances = all_results[f\"{dataset_name}_{model_name}_{attack_name}\"][\"result\"]['average_distances']\n",
    "    l0_distances = average_distances['L0 Distance']\n",
    "    l1_distances = average_distances['L1 Distance']\n",
    "    l2_distances = average_distances['L2 Distance']\n",
    "    linf_distances = average_distances['Linf Distance']\n",
    "    mahalanobis_distances = average_distances['Mahalanobis Distance']\n",
    "    sensitivity = average_distances['Sensitivity']\n",
    "    outliner_rates = all_results[f\"{dataset_name}_{model_name}_{attack_name}\"][\"result\"]['outliner_rates']\n",
    "    epsilons = all_results[f\"{dataset_name}_{model_name}_{attack_name}\"][\"result\"]['epsilons']\n",
    "    paths = all_results[f\"{dataset_name}_{model_name}_{attack_name}\"][\"result\"]['numpy_path']\n",
    "\n",
    "    fig, ax = plt.subplots(3, 3 ,figsize=(18, 14))\n",
    "    fig.suptitle(f'Model: {model_name} - Dataset: {dataset_name} - Attack: {attack_name}', y=0.92, fontsize=16)\n",
    "    ax[0, 0].plot(epsilons, attack_success_rates, marker='o', linestyle='-')\n",
    "    ax[0, 0].set_title(f'Success Rate vs. Epsilon')\n",
    "    ax[0, 0].set_xlabel('Epsilon')\n",
    "    ax[0, 0].set_ylabel('Success Rate')\n",
    "    ax[0, 0].grid(True)\n",
    "\n",
    "    ax[0, 1].plot(epsilons, l0_distances, marker='o', linestyle='-')\n",
    "    ax[0, 1].set_title(f'Sparsity vs. Epsilon')\n",
    "    ax[0, 1].set_xlabel('Epsilon')\n",
    "    ax[0, 1].set_ylabel('Sparsity')\n",
    "    ax[0, 1].grid(True)\n",
    "\n",
    "    ax[0, 2].plot(epsilons, l1_distances, marker='o', linestyle='-')\n",
    "    ax[0, 2].set_title(f'L1 Distance vs. Epsilon')\n",
    "    ax[0, 2].set_xlabel('Epsilon')\n",
    "    ax[0, 2].set_ylabel('L1 Distance')\n",
    "    ax[0, 2].grid(True)\n",
    "\n",
    "    ax[1, 0].plot(epsilons, l2_distances, marker='o', linestyle='-')\n",
    "    ax[1, 0].set_title(f'L2 Distance vs. Epsilon')\n",
    "    ax[1, 0].set_xlabel('Epsilon')\n",
    "    ax[1, 0].set_ylabel('L2 Distance')\n",
    "    ax[1, 0].grid(True)\n",
    "\n",
    "    ax[1, 1].plot(epsilons, linf_distances, marker='o', linestyle='-')\n",
    "    ax[1, 1].set_title(f'Linf Distance vs. Epsilon')\n",
    "    ax[1, 1].set_xlabel('Epsilon')\n",
    "    ax[1, 1].set_ylabel('Linf Distance')\n",
    "    ax[1, 1].grid(True)\n",
    "\n",
    "    ax[1, 2].plot(epsilons, mahalanobis_distances, marker='o', linestyle='-')\n",
    "    ax[1, 2].set_title(f'Mahalanobis Distance vs. Epsilon')\n",
    "    ax[1, 2].set_xlabel('Epsilon')\n",
    "    ax[1, 2].set_ylabel('Mahalanobis Distance')\n",
    "    ax[1, 2].grid(True)\n",
    "\n",
    "    ax[2, 0].plot(epsilons, outliner_rates, marker='o', linestyle='-')\n",
    "    ax[2, 0].set_title(f'Outlier Rate vs. Epsilon')\n",
    "    ax[2, 0].set_xlabel('Epsilon')\n",
    "    ax[2, 0].set_ylabel('Outlier Rate')\n",
    "    ax[2, 0].grid(True)\n",
    "\n",
    "    ax[2, 1].plot(epsilons, sensitivity, marker='o', linestyle='-')\n",
    "    ax[2, 1].set_title(f'Sensitivity vs. Epsilon')\n",
    "    ax[2, 1].set_xlabel('Epsilon')\n",
    "    ax[2, 1].set_ylabel('Sensitivity')\n",
    "    ax[2, 1].grid(True)\n",
    "\n",
    "    # ax[2, 2].plot(l2_distances, attack_success_rates, marker='o', linestyle='-')\n",
    "    # ax[2, 2].set_title(f'L2 Distance vs. Success Rate')\n",
    "    # ax[2, 2].set_xlabel('L2 Distance')\n",
    "    # ax[2, 2].set_ylabel('Success Rate')\n",
    "    # ax[2, 2].grid(True)\n",
    "\n",
    "    matrix_list = []\n",
    "    for i in range(len(epsilons)):\n",
    "        success_arr = np.load(paths[i][\"success_arr\"])\n",
    "        success_arr_adv = np.load(paths[i][\"success_arr_adv\"])\n",
    "\n",
    "        matrix = calculate_perturbated_feature(success_arr, success_arr_adv)\n",
    "        matrix_list.append(matrix)\n",
    "\n",
    "    ax[2, 2].imshow(matrix_list, cmap='viridis',aspect='auto')\n",
    "    # Add grid lines for each cell\n",
    "    ax[2, 2].grid(which='both', color='grey', linewidth=0.1)\n",
    "    # ax[2, 2].colorbar(label='Values')\n",
    "    ax[2, 2].set_title(f'Heatmap of Perturbated Features')\n",
    "    ax[2, 2].set_xlabel('Features')\n",
    "    ax[2, 2].set_ylabel('Epsilon')\n",
    "\n",
    "    plt.show()\n",
    "    os.makedirs(\"results/figures\", exist_ok=True)\n",
    "    fig.savefig(f\"results/figures/{dataset_name}_{model_name}_{attack_name}.png\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset_name in dataset_list:\n",
    "    for model_name in model_list:\n",
    "        for attack_name in attack_list:\n",
    "            if all_results[f\"{dataset_name}_{model_name}_{attack_name}\"] is not None:\n",
    "                plot_in_grid(all_results, dataset_name, model_name, attack_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot different models and different attacks in one figure for comparison (same dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_multiline(all_results, dataset_name, models, attacks):\n",
    "    fig, ax = plt.subplots(4, 3 ,figsize=(18, 19))\n",
    "\n",
    "    for model_name in models:\n",
    "        for attack_name in attacks:\n",
    "            if all_results[f\"{dataset_name}_{model_name}_{attack_name}\"] is None:\n",
    "                continue\n",
    "            \n",
    "            attack_success_rates = all_results[f\"{dataset_name}_{model_name}_{attack_name}\"][\"result\"]['attack_success_rates']\n",
    "            average_distances = all_results[f\"{dataset_name}_{model_name}_{attack_name}\"][\"result\"]['average_distances']\n",
    "            l0_distances = average_distances['L0 Distance']\n",
    "            l1_distances = average_distances['L1 Distance']\n",
    "            l2_distances = average_distances['L2 Distance']\n",
    "            linf_distances = average_distances['Linf Distance']\n",
    "            mahalanobis_distances = average_distances['Mahalanobis Distance']\n",
    "            sensitivity = average_distances['Sensitivity']\n",
    "            outliner_rates = all_results[f\"{dataset_name}_{model_name}_{attack_name}\"][\"result\"]['outliner_rates']\n",
    "            epsilons = all_results[f\"{dataset_name}_{model_name}_{attack_name}\"][\"result\"]['epsilons']\n",
    "            paths = all_results[f\"{dataset_name}_{model_name}_{attack_name}\"][\"result\"]['numpy_path']\n",
    "\n",
    "            ax[0, 0].plot(epsilons, attack_success_rates, marker='o', linestyle='-', label=f\"{model_name}/{attack_name}\")\n",
    "\n",
    "            ax[0, 1].plot(epsilons, l0_distances, marker='o', linestyle='-', label=f\"{model_name}/{attack_name}\")\n",
    "\n",
    "            ax[0, 2].plot(epsilons, l1_distances, marker='o', linestyle='-', label=f\"{model_name}/{attack_name}\")\n",
    "\n",
    "            ax[1, 0].plot(epsilons, l2_distances, marker='o', linestyle='-', label=f\"{model_name}/{attack_name}\")\n",
    "\n",
    "            ax[1, 1].plot(epsilons, linf_distances, marker='o', linestyle='-', label=f\"{model_name}/{attack_name}\")\n",
    "\n",
    "            ax[1, 2].plot(epsilons, mahalanobis_distances, marker='o', linestyle='-', label=f\"{model_name}/{attack_name}\")\n",
    "\n",
    "            ax[2, 0].plot(epsilons, outliner_rates, marker='o', linestyle='-', label=f\"{model_name}/{attack_name}\")\n",
    "\n",
    "            ax[2, 1].plot(epsilons, sensitivity, marker='o', linestyle='-', label=f\"{model_name}/{attack_name}\")\n",
    "\n",
    "            ax[2, 2].plot(l0_distances, attack_success_rates, marker='o', linestyle='-', label=f\"{model_name}/{attack_name}\")\n",
    "\n",
    "            ax[3, 0].plot(l2_distances, attack_success_rates, marker='o', linestyle='-', label=f\"{model_name}/{attack_name}\")\n",
    "\n",
    "            ax[3, 1].plot(linf_distances, attack_success_rates, marker='o', linestyle='-', label=f\"{model_name}/{attack_name}\")\n",
    "\n",
    "            ax[3, 2].plot(outliner_rates, attack_success_rates, marker='o', linestyle='-', label=f\"{model_name}/{attack_name}\")\n",
    "\n",
    "\n",
    "    ax[0, 0].set_title(f'Success Rate vs. Epsilon')\n",
    "    ax[0, 0].set_xlabel('Epsilon')\n",
    "    ax[0, 0].set_ylabel('Success Rate')\n",
    "    # ax[0, 0].legend()\n",
    "    ax[0, 0].grid(True)\n",
    "\n",
    "    ax[0, 1].set_title(f'Sparsity vs. Epsilon')\n",
    "    ax[0, 1].set_xlabel('Epsilon')\n",
    "    ax[0, 1].set_ylabel('Sparsity')\n",
    "    # ax[0, 1].legend()\n",
    "    ax[0, 1].grid(True)\n",
    "\n",
    "    ax[0, 2].set_title(f'L1 Distance vs. Epsilon')\n",
    "    ax[0, 2].set_xlabel('Epsilon')\n",
    "    ax[0, 2].set_ylabel('L1 Distance')\n",
    "    # ax[0, 2].legend()\n",
    "    ax[0, 2].grid(True)\n",
    "\n",
    "    ax[1, 0].set_title(f'L2 Distance vs. Epsilon')\n",
    "    ax[1, 0].set_xlabel('Epsilon')\n",
    "    ax[1, 0].set_ylabel('L2 Distance')\n",
    "    # ax[1, 0].legend()\n",
    "    ax[1, 0].grid(True)\n",
    "\n",
    "    ax[1, 1].set_title(f'Linf Distance vs. Epsilon')\n",
    "    ax[1, 1].set_xlabel('Epsilon')\n",
    "    ax[1, 1].set_ylabel('Linf Distance')\n",
    "    # ax[1, 1].legend()\n",
    "    ax[1, 1].grid(True)\n",
    "\n",
    "    ax[1, 2].set_title(f'Mahalanobis Distance vs. Epsilon')\n",
    "    ax[1, 2].set_xlabel('Epsilon')\n",
    "    ax[1, 2].set_ylabel('Mahalanobis Distance')\n",
    "    # ax[1, 2].legend()\n",
    "    ax[1, 2].grid(True)\n",
    "\n",
    "    ax[2, 0].set_title(f'Outlier Rate vs. Epsilon')\n",
    "    ax[2, 0].set_xlabel('Epsilon')\n",
    "    ax[2, 0].set_ylabel('Outlier Rate')\n",
    "    # ax[2, 0].legend()\n",
    "    ax[2, 0].grid(True)\n",
    "\n",
    "    ax[2, 1].set_title(f'Sensitivity vs. Epsilon')\n",
    "    ax[2, 1].set_xlabel('Epsilon')\n",
    "    ax[2, 1].set_ylabel('Sensitivity')\n",
    "    # ax[2, 1].legend()\n",
    "    ax[2, 1].grid(True)\n",
    "\n",
    "    ax[2, 2].set_title(f'Sparsity vs. Success Rate')\n",
    "    ax[2, 2].set_xlabel('L0 Distance')\n",
    "    ax[2, 2].set_ylabel('Success Rate')\n",
    "    # ax[2, 2].legend()\n",
    "    ax[2, 2].grid(True)\n",
    "\n",
    "    ax[3, 0].set_title(f'L2 Distance vs. Success Rate')\n",
    "    ax[3, 0].set_xlabel('L2 Distance')\n",
    "    ax[3, 0].set_ylabel('Success Rate')\n",
    "    # ax[3, 0].legend()\n",
    "    ax[3, 0].grid(True)\n",
    "\n",
    "    ax[3, 1].set_title(f'Linf Distance vs. Success Rate')\n",
    "    ax[3, 1].set_xlabel('Linf Distance')\n",
    "    ax[3, 1].set_ylabel('Success Rate')\n",
    "    # ax[3, 1].legend()\n",
    "    ax[3, 1].grid(True)\n",
    "\n",
    "    ax[3, 2].set_title(f'Outliner Rate vs. Success Rate')\n",
    "    ax[3, 2].set_xlabel('Outliner Rate')\n",
    "    ax[3, 2].set_ylabel('Success Rate')\n",
    "    # ax[3, 2].legend()\n",
    "    ax[3, 2].grid(True)\n",
    "\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1.0), loc='lower left')\n",
    "\n",
    "\n",
    "\n",
    "    os.makedirs(\"results/figures\", exist_ok=True)\n",
    "    if len(models) > 1:\n",
    "        model_n = \"All\"\n",
    "    else:\n",
    "        model_n = models[0]\n",
    "    if len(attacks) > 1:\n",
    "        attack_n = \"All\"\n",
    "    else:\n",
    "        attack_n = attacks[0]\n",
    "\n",
    "    fig.suptitle(f'Dataset: {dataset_name} | Model: {model_n} | Attack: {attack_n}' , y=0.92, fontsize=16)\n",
    "\n",
    "    fig.savefig(f\"results/figures/{dataset_name}_{model_n}_{attack_n}.png\", bbox_inches = 'tight')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset_name in dataset_list:\n",
    "    plot_multiline(all_results, dataset_name, model_list, attack_list)\n",
    "    for model_name in model_list:\n",
    "        plot_multiline(all_results, dataset_name, [model_name], attack_list)\n",
    "    for attack_name in attack_list:\n",
    "        plot_multiline(all_results, dataset_name, model_list, [attack_name])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additional: PCA to visualize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import the necessary libraries\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Step 2: Load and preprocess the data\n",
    "\n",
    "success_arr = np.load(paths[idx][\"success_arr\"])\n",
    "success_arr_adv = np.load(paths[idx][\"success_arr_adv\"])\n",
    "\n",
    "\n",
    "combined_array = np.vstack((success_arr, success_arr_adv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Apply dimensionality reduction using PCA\n",
    "n_components = 2  # Choose the number of components for visualization (here, 2 for plotting)\n",
    "pca = PCA(n_components=n_components)\n",
    "# tsne = TSNE(n_components=n_components)\n",
    "\n",
    "# Fit PCA on the combined data\n",
    "pca.fit(combined_array)\n",
    "# combined_tsne = tsne.fit_transform(combined_array)\n",
    "\n",
    "# Apply PCA to array1 and array2\n",
    "success_arr_pca = pca.transform(success_arr)\n",
    "success_arr_adv_pca = pca.transform(success_arr_adv)\n",
    "# success_arr_tsne = combined_tsne[:len(success_arr)]\n",
    "# success_arr_adv_tsne = combined_tsne[:len(success_arr_adv)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Plot the first 100 rows of each array in a dot plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "num = 100\n",
    "\n",
    "# Plot the first 100 rows of array1\n",
    "plt.scatter(success_arr_pca[:num, 0], success_arr_pca[:num, 1], color='blue', label='Original (First 100)')\n",
    "\n",
    "# Plot the first 100 rows of array2\n",
    "plt.scatter(success_arr_adv_pca[:num, 0], success_arr_adv_pca[:num, 1], color='red', label='Adversarial Examples (First 100)')\n",
    "\n",
    "\n",
    "# Add arrows from array1_pca to array2_pca for the first 100 points\n",
    "for i in range(num):\n",
    "    plt.arrow(success_arr_pca[i, 0], success_arr_pca[i, 1],\n",
    "              success_arr_adv_pca[i, 0] - success_arr_pca[i, 0], success_arr_adv_pca[i, 1] - success_arr_pca[i, 1],\n",
    "              color='gray', alpha=0.5, width=0.002, head_width=0.04)\n",
    "\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.legend()\n",
    "plt.title('PCA Plot of Original and Success Adversarial Examples (First 100) - With Categorical Features')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tabular-benchmark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
